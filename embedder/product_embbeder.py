import os
import json
import hashlib
from datetime import datetime, timezone
from PIL import Image
from sentence_transformers import SentenceTransformer

# Work around oneDNN/PIR issues on some Windows CPU installs.
os.environ.setdefault("FLAGS_use_onednn", "false")

from paddleocr import PaddleOCR
import chromadb

# ==========================================
# 1. ì„¤ì • (Configuration)
# ==========================================
JSON_FILE = "../crawl/example.json"  # ìµœì‹  ë°ì´í„° íŒŒì¼ ê²½ë¡œ
IMAGE_DIR = "../crawl/images"
DB_PATH = "./chroma_db"
COLLECTION_NAME = "rakuten_products"
DEBUG_OCR = True

def run_embedding():
    print("ğŸš€ ìŠ¤ë§ˆíŠ¸ ì„ë² ë”© ì‹œìŠ¤í…œ ê°€ë™ (ì¤‘ë³µ ë°©ì§€ & ì´ë¯¸ì§€ ê²€ì¦ í¬í•¨)...")

    # 1. DB ì—°ê²°
    client = chromadb.PersistentClient(path=DB_PATH)
    collection = client.get_or_create_collection(name=COLLECTION_NAME)

    # 2. ëª¨ë¸ ë¡œë“œ
    print("ğŸ“¥ CLIP ëª¨ë¸ ë¡œë”© ì¤‘...")
    model = SentenceTransformer('clip-ViT-B-32')

    print("ğŸ“¥ PaddleOCR ë¡œë”© ì¤‘...")
    ocr = PaddleOCR(use_textline_orientation=True, lang='japan')

    # 3. JSON ë°ì´í„° ë¡œë“œ
    try:
        with open(JSON_FILE, 'r', encoding='utf-8') as f:
            products = json.load(f)
    except FileNotFoundError:
        print(f"âŒ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {JSON_FILE}")
        return

    print(f"ğŸ“¦ ì²˜ë¦¬ ëŒ€ìƒ ìƒí’ˆ: {len(products)}ê°œ")

    # ì´ë¯¸ DBì— ì €ì¥ëœ ID ëª©ë¡ì„ í•„ìš”í•œ ë§Œí¼ë§Œ ì¡°íšŒí•©ë‹ˆë‹¤. (ì¤‘ë³µ ë°©ì§€ìš©)
    # ì „ì²´ ë°ì´í„°ë¥¼ í†µì§¸ë¡œ ê°€ì ¸ì˜¤ì§€ ì•Šê³ , í˜„ì¬ ì²˜ë¦¬ ëŒ€ìƒ IDë§Œ ë°°ì¹˜ ì¡°íšŒí•©ë‹ˆë‹¤.
    existing_ids = set()
    existing_meta_by_id = {}
    candidate_ids = [str(item['id']) for item in products if 'id' in item]
    chunk_size = 1000
    for i in range(0, len(candidate_ids), chunk_size):
        chunk = candidate_ids[i:i + chunk_size]
        try:
            found = collection.get(ids=chunk, include=["ids", "metadatas"])
            ids = found.get('ids', [])
            existing_ids.update(ids)
            for idx, meta in enumerate(found.get('metadatas', [])):
                if idx < len(ids):
                    existing_meta_by_id[ids[idx]] = meta or {}
        except Exception:
            # ì¼ë¶€ ë²„ì „ì—ì„œëŠ” include ì¸ìê°€ ì œí•œë  ìˆ˜ ìˆì–´ ì•ˆì „í•˜ê²Œ ì¬ì‹œë„
            found = collection.get(ids=chunk)
            ids = found.get('ids', [])
            existing_ids.update(ids)
            for idx, meta in enumerate(found.get('metadatas', [])):
                if idx < len(ids):
                    existing_meta_by_id[ids[idx]] = meta or {}

    print(f"ğŸ’¾ í˜„ì¬ DB ì €ì¥ëœ ìƒí’ˆ ìˆ˜(ëŒ€ìƒ ê¸°ì¤€): {len(existing_ids)}ê°œ")

    # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì„ì‹œ ì €ì¥ì†Œ
    batch_ids = []
    batch_embeddings = []
    batch_metadatas = []
    
    new_count = 0
    update_count = 0
    skip_count = 0
    error_count = 0

    # 4. ì´ë¯¸ì§€ ì¸ë±ìŠ¤ ìƒì„± (O(ì´ë¯¸ì§€ ìˆ˜) í•œ ë²ˆë§Œ)
    image_index = {}
    for f_name in os.listdir(IMAGE_DIR):
        if f_name.endswith('.jpg') or f_name.endswith('.png'):
            product_id = f_name.split('_', 1)[0].split('.', 1)[0]
            if product_id not in image_index:
                image_index[product_id] = f_name

    # 5. ë°ì´í„° ìˆœíšŒ
    debug_printed = 0
    for idx, item in enumerate(products):
        product_id = str(item['id'])
        
        # [ì²´í¬ 2] ì´ë¯¸ì§€ê°€ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ê°€?
        # JSONì—ëŠ” íŒŒì¼ëª…ì´ ì—†ìœ¼ë¯€ë¡œ IDë¡œ ì‹œì‘í•˜ëŠ” ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤.
        image_filename = image_index.get(product_id)
        
        if not image_filename:
            # print(f"   âš ï¸ ì´ë¯¸ì§€ íŒŒì¼ ì—†ìŒ (Skip): {item['name']}")
            error_count += 1
            continue

        image_path = os.path.join(IMAGE_DIR, image_filename)
        image_hash = _hash_file(image_path)

        try:
            now_iso = datetime.now(timezone.utc).isoformat()
            existing_meta = existing_meta_by_id.get(product_id)
            created_at = (existing_meta or {}).get("created_at") or now_iso
            metadata_base = {
                "name": item['name'],
                "price": item['price'],
                "maker": item['maker'],
                "category": item['category'],
                "image_path": image_path, # ë‚˜ì¤‘ì— ì›¹ì—ì„œ ë³´ì—¬ì¤„ ë•Œ í•„ìš”
                "product_url": item['product_url'],
                "image_hash": image_hash,
                "created_at": created_at,
                "updated_at": now_iso,
            }

            if product_id in existing_ids:
                changed = False
                if existing_meta:
                    compare_keys = [
                        "name",
                        "price",
                        "maker",
                        "category",
                        "product_url",
                        "image_hash",
                    ]
                    for key in compare_keys:
                        if existing_meta.get(key) != metadata_base.get(key):
                            changed = True
                            break
                else:
                    changed = True

                if not changed:
                    skip_count += 1
                    continue

                # [ì²´í¬ 3] ì´ë¯¸ì§€ê°€ ê¹¨ì§€ì§€ ì•Šê³  ì—´ë¦¬ëŠ”ê°€? (Validation)
                with Image.open(image_path) as img:
                    # ì´ë¯¸ì§€ë¥¼ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆê²Œ ë²¡í„°ë¡œ ë³€í™˜
                    vector = model.encode(img).tolist()

                ocr_lines = _run_ocr(ocr, image_path)
                if DEBUG_OCR:
                    print(f"   ğŸ§ª OCR ë””ë²„ê·¸: {product_id} lines={len(ocr_lines)}")
                    for line in ocr_lines[:3]:
                        print(f"      - {line}")
                    debug_printed += 1
                metadata = dict(metadata_base)
                metadata["ocr_lines"] = _serialize_ocr_lines(ocr_lines)

                batch_ids.append(product_id)
                batch_embeddings.append(vector)
                batch_metadatas.append(metadata)
                update_count += 1
                print(f"   ğŸ” [ê°±ì‹ ] {item['name'][:15]}... ì—…ë°ì´íŠ¸ë¨")
            else:
                # [ì²´í¬ 3] ì´ë¯¸ì§€ê°€ ê¹¨ì§€ì§€ ì•Šê³  ì—´ë¦¬ëŠ”ê°€? (Validation)
                with Image.open(image_path) as img:
                    # ì´ë¯¸ì§€ë¥¼ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆê²Œ ë²¡í„°ë¡œ ë³€í™˜
                    vector = model.encode(img).tolist()

                ocr_lines = _run_ocr(ocr, image_path)
                if DEBUG_OCR:
                    print(f"   ğŸ§ª OCR ë””ë²„ê·¸: {product_id} lines={len(ocr_lines)}")
                    for line in ocr_lines[:3]:
                        print(f"      - {line}")
                    debug_printed += 1
                metadata = dict(metadata_base)
                metadata["ocr_lines"] = _serialize_ocr_lines(ocr_lines)

                batch_ids.append(product_id)
                batch_embeddings.append(vector)
                batch_metadatas.append(metadata)
                new_count += 1
                print(f"   âœ… [ì‹ ê·œ] {item['name'][:15]}... ì¶”ê°€ë¨")

        except Exception as e:
            print(f"   âŒ ì´ë¯¸ì§€ ì†ìƒ ë˜ëŠ” ì—ëŸ¬ ({item['name']}): {e}")
            error_count += 1
            continue

    # 5. DBì— ì €ì¥ (ì‹ ê·œ/ê°±ì‹  ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ)
    if batch_ids:
        print(f"\nğŸ“¥ ì‹ ê·œ/ê°±ì‹  ë°ì´í„° {len(batch_ids)}ê°œë¥¼ DBì— ì €ì¥í•©ë‹ˆë‹¤...")
        collection.upsert(
            ids=batch_ids,
            embeddings=batch_embeddings,
            metadatas=batch_metadatas
        )
        print("ğŸ‰ ì €ì¥ ì™„ë£Œ!")
    else:
        print("\nâœ¨ ì¶”ê°€í•  ì‹ ê·œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # 6. ìµœì¢… ë¦¬í¬íŠ¸
    print("\n" + "="*30)
    print(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½")
    print(f"   - ì´ ë°ì´í„°: {len(products)}")
    print(f"   - ì´ë¯¸ ì¡´ì¬í•¨ (Skip): {skip_count}")
    print(f"   - ì´ë¯¸ì§€ ì˜¤ë¥˜/ì—†ìŒ: {error_count}")
    print(f"   - ìƒˆë¡œ ì¶”ê°€ë¨: {new_count}")
    print(f"   - ê°±ì‹ ë¨: {update_count}")
    print("="*30)

def _run_ocr(ocr: PaddleOCR, image_path: str) -> list:
    lines = []

    try:
        result = ocr.predict(input=image_path)
        for res in result:
            lines.extend(_extract_ocr_lines(res))
    except Exception:
        return []

    return lines


def _extract_ocr_lines(result) -> list:
    if isinstance(result, dict):
        texts = result.get("rec_texts") or result.get("texts")
        scores = result.get("rec_scores") or result.get("scores")
        if texts:
            lines = []
            for text, score in zip(texts, scores or [None] * len(texts)):
                clean_text = str(text).strip()
                clean_score = float(score) if score is not None else None
                if not clean_text:
                    continue
                if clean_score is not None and clean_score <= 0.0:
                    continue
                lines.append({"text": clean_text, "score": clean_score})
            return lines
        if "text" in result:
            return [{"text": result.get("text"), "score": result.get("score")}]
        return []

    for attr in ("to_json", "json"):
        if hasattr(result, attr):
            try:
                data = getattr(result, attr)()
                return _extract_ocr_lines(data)
            except Exception:
                pass

    if isinstance(result, list):
        if len(result) == 1 and isinstance(result[0], list):
            result = result[0]
        lines = []
        for line in result:
            if isinstance(line, (list, tuple)) and len(line) >= 2:
                payload = line[1]
                if isinstance(payload, (list, tuple)) and len(payload) >= 2:
                    text = str(payload[0]).strip()
                    score = float(payload[1]) if payload[1] is not None else None
                    if not text:
                        continue
                    if score is not None and score <= 0.0:
                        continue
                    lines.append({"text": text, "score": score})
        return lines

    return []


def _serialize_ocr_lines(lines: list) -> str:
    if not lines:
        return ""
    try:
        return json.dumps(lines, ensure_ascii=False)
    except Exception:
        return ""


def _hash_file(file_path: str) -> str:
    hasher = hashlib.sha256()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(1024 * 1024), b""):
            hasher.update(chunk)
    return hasher.hexdigest()


if __name__ == "__main__":
    run_embedding()