from brand_mapping_data import BRAND_MAPPING, get_official_maker_name
from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from pydantic import BaseModel
from typing import Optional
from PIL import Image
import io
import json
import os
import chromadb
from sentence_transformers import SentenceTransformer
from paddleocr import PaddleOCR
import numpy as np
import uvicorn
import traceback
from pathlib import Path
from scipy.spatial.distance import cosine
from difflib import SequenceMatcher

# ==========================================
# 1. ì„¤ì • ë° ëª¨ë¸ ë¡œë“œ
# ==========================================
DB_PATH = "../embedder/chroma_db"
COLLECTION_NAME = "rakuten_products"
RESPONSE_JSON_PATH = os.path.join(os.path.dirname(__file__), "response.json")
DEBUG_SCORING = True
DEBUG_SCORING_LIMIT = 5

def similarity(a: str, b: str) -> float:
    """ë‘ ë¬¸ìì—´ì˜ ìœ ì‚¬ë„ ê³„ì‚° (0.0~1.0)"""
    return SequenceMatcher(None, a.lower(), b.lower()).ratio()

def _load_env_file() -> None:
    env_path = Path(__file__).resolve().parent / ".env"
    if not env_path.exists():
        return

    for raw_line in env_path.read_text(encoding="utf-8").splitlines():
        line = raw_line.strip()
        if not line or line.startswith("#") or "=" not in line:
            continue
        key, value = line.split("=", 1)
        key = key.strip()
        value = value.strip().strip("'\"")
        os.environ.setdefault(key, value)


def _get_float(name: str, default: float) -> float:
    value = os.getenv(name)
    if value is None:
        return default
    try:
        return float(value)
    except ValueError:
        return default


def _get_int(name: str, default: int) -> int:
    value = os.getenv(name)
    if value is None:
        return default
    try:
        return int(value)
    except ValueError:
        return default


def load_weights() -> dict:
    _load_env_file()
    weights = {
        "base_score_weight": _get_float("MONOLOG_BASE_SCORE_WEIGHT", 0.3),
        "brand_bonus": _get_float("MONOLOG_BRAND_BONUS", 0.15),
        "name_bonus": _get_float("MONOLOG_NAME_BONUS", 0.15),
        "ocr_threshold_minimum": _get_int("MONOLOG_OCR_THRESHOLD_MINIMUM", 10),
        "ocr_threshold_fair": _get_int("MONOLOG_OCR_THRESHOLD_FAIR", 30),
        "ocr_threshold_good": _get_int("MONOLOG_OCR_THRESHOLD_GOOD", 60),
        "ocr_bonus_poor": _get_float("MONOLOG_OCR_BONUS_POOR", 0.0),
        "ocr_bonus_fair": _get_float("MONOLOG_OCR_BONUS_FAIR", 0.025),
        "ocr_bonus_good": _get_float("MONOLOG_OCR_BONUS_GOOD", 0.05),
        "price_bonus_10pct": _get_float("MONOLOG_PRICE_BONUS_10PCT", 0.10),
        "price_bonus_20pct": _get_float("MONOLOG_PRICE_BONUS_20PCT", 0.05),
        "price_threshold_10pct": _get_int("MONOLOG_PRICE_THRESHOLD_10PCT", 10),
        "price_threshold_20pct": _get_int("MONOLOG_PRICE_THRESHOLD_20PCT", 20),
        "similarity_threshold": _get_float("MONOLOG_SIMILARITY_THRESHOLD", 0.8),
    }
    
    # ìµœëŒ€ ê°€ëŠ¥í•œ ë³´ë„ˆìŠ¤ í•©ê³„ ê³„ì‚° (ëª¨ë“  ì¡°ê±´ ë§Œì¡±)
    weights["max_bonus"] = (
        weights["brand_bonus"] +  # ë¸Œëœë“œ ì¼ì¹˜
        weights["name_bonus"] +  # ì œí’ˆëª… ì¼ì¹˜
        weights["ocr_bonus_good"] +  # OCR ìš°ìˆ˜
        weights["price_bonus_10pct"]  # ê°€ê²© 10% ì´ë‚´
    )
    
    return weights


WEIGHTS = load_weights()
SIMILARITY_THRESHOLD = WEIGHTS.get("similarity_threshold", 0.8)

app = FastAPI(title="Mono-Log AI Server", description="ì´ë¯¸ì§€ ì¤‘ì‹¬ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì—”ì§„")

# ëª¨ë¸ì€ ì²« ìš”ì²­ ì‹œ ë¡œë“œ (lazy loading)
model = None
ocr = None
client = None
collection = None

def initialize_models():
    """ëª¨ë¸ ì´ˆê¸°í™” (ì²« ìš”ì²­ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰)"""
    global model, ocr, client, collection
    if model is None:
        print("â³ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        import time
        
        start = time.time()
        model = SentenceTransformer('clip-ViT-B-32')
        print(f"  âœ“ CLIP ëª¨ë¸ ë¡œë“œ: {time.time()-start:.2f}ì´ˆ")
        
        start = time.time()
        ocr = PaddleOCR(use_textline_orientation=True, lang='japan')
        print(f"  âœ“ OCR ì—”ì§„ ë¡œë“œ: {time.time()-start:.2f}ì´ˆ")
        
        start = time.time()
        client = chromadb.PersistentClient(path=DB_PATH)
        collection = client.get_collection(name=COLLECTION_NAME)
        print(f"  âœ“ DB ì—°ê²°: {time.time()-start:.2f}ì´ˆ")
        
        print("âœ… ì„œë²„ ì¤€ë¹„ ì™„ë£Œ!")

# ==========================================
# 2. í•µì‹¬ ë¡œì§: ê°€ì¤‘ì¹˜ ê³„ì‚° í•¨ìˆ˜
# ==========================================
def calculate_final_score(item, user_inputs, detected_texts=None):
    # 1. ê¸°ë³¸ ì ìˆ˜: ì´ë¯¸ì§€ ë²¡í„° ìœ ì‚¬ë„ (0.0 ~ 1.0)
    base_score = item['similarity_score']
    
    # 2. ê°€ì¤‘ì¹˜ ì ìˆ˜ í•©ì‚°
    bonus_score = 0.0
    
    # [í•„í„° 1] ë¸Œëœë“œ (ê°€ì¥ ê°•ë ¥í•œ íŒíŠ¸)
    brand_matched = False
    user_brand = user_inputs.get('brand')
    if user_brand:
        # ì…ë ¥ê°’ì„ ì†Œë¬¸ìë¡œ ë°”ê¾¸ê³ , ë§¤í•‘ëœ ì¼ë³¸ì–´ê°€ ìˆìœ¼ë©´ ê°€ì ¸ì˜´
        target_maker_keyword = get_official_maker_name(user_brand)
        
        # DBì˜ ì œì¡°ì‚¬(maker) ì •ë³´ì™€ ë¹„êµ (ë¶€ë¶„ ì¼ì¹˜)
        # ì˜ˆ: 'nissin' -> 'æ—¥æ¸…' ë°˜í™˜ -> DBì˜ 'æ—¥æ¸…é£Ÿå“'ì— í¬í•¨ë˜ë¯€ë¡œ OK!
        if target_maker_keyword in item.get('maker', ''):
            bonus_score += WEIGHTS["brand_bonus"]
            brand_matched = True
    
    # [í•„í„° 1-2] OCRì—ì„œ ë¸Œëœë“œëª… ë°œê²¬ (user_brand ì—†ì–´ë„ ì‘ë™)
    if not brand_matched and detected_texts:
        detected_full = ' '.join(detected_texts)
        item_maker = item.get('maker', '')
        # ì™„ì „ ì¼ì¹˜ ì²´í¬
        if item_maker and item_maker in detected_full:
            bonus_score += WEIGHTS["brand_bonus"]
            brand_matched = True
        # ìœ ì‚¬ë„ ì²´í¬ (OCR ì˜¤ë¥˜ ëŒ€ì‘: HISSIN vs NISSIN)
        elif item_maker:
            for word in detected_texts:
                if len(word) >= 3 and similarity(word, item_maker) >= SIMILARITY_THRESHOLD:
                    bonus_score += WEIGHTS["brand_bonus"]
                    brand_matched = True
                    break

    # [í•„í„° 2] ì œí’ˆëª… (name íŒŒë¼ë¯¸í„° ë˜ëŠ” OCR ìë™ ê°ì§€)
    name_matched = False
    user_name = user_inputs.get('name')
    if user_name:
        # API name ì…ë ¥: DB nameì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸
        if user_name.lower() in item.get('name', '').lower():
            bonus_score += WEIGHTS["name_bonus"]
            name_matched = True
    
    # OCRì—ì„œ ì œí’ˆëª… ìë™ ê°ì§€
    if not name_matched and detected_texts:
        detected_full = ' '.join(detected_texts)
        item_name = item.get('name', '')
        # ì™„ì „ ì¼ì¹˜ ì²´í¬
        if item_name and (item_name in detected_full or 
                          any(word in item_name for word in detected_texts if len(word) >= 2)):
            bonus_score += WEIGHTS["name_bonus"]
            name_matched = True
        # ìœ ì‚¬ë„ ì²´í¬ (OCR ì˜¤ë¥˜ ëŒ€ì‘)
        elif item_name:
            for word in detected_texts:
                if len(word) >= 3 and similarity(word, item_name) >= SIMILARITY_THRESHOLD:
                    bonus_score += WEIGHTS["name_bonus"]
                    name_matched = True
                    break

    # [í•„í„° 3] ê°€ê²© (ë¹„ìŠ·í•˜ë©´ ì ìˆ˜)
    user_price = user_inputs.get('price')
    if user_price:
        try:
            target_price = int(user_price)
            item_price = int(item.get('price', 0))
            diff = abs(target_price - item_price)
            price_ratio = (diff / target_price * 100) if target_price > 0 else 100
            if price_ratio <= WEIGHTS["price_threshold_10pct"]:
                bonus_score += WEIGHTS["price_bonus_10pct"]
            elif price_ratio <= WEIGHTS["price_threshold_20pct"]:
                bonus_score += WEIGHTS["price_bonus_20pct"]
        except:
            pass

    # [í•„í„° 4] OCR ì¼ì¹˜ìœ¨ (ì—…ë¡œë“œ ì´ë¯¸ì§€ OCRê³¼ DB ocr_lines ë¹„êµ)
    if detected_texts:
        _, ocr_bonus = _calculate_ocr_match_score(detected_texts, item, debug_ocr=False)
        bonus_score += ocr_bonus

    # ì •ê·œí™”: 0~1 ë²”ìœ„ë¡œ ë³€í™˜
    # ìµœëŒ€ ê°€ëŠ¥ ì ìˆ˜ = base(1.0) * BASE_WEIGHT + max_bonus * BONUS_WEIGHT
    bonus_weight = 1.0 - WEIGHTS["base_score_weight"]
    max_possible = 1.0 * WEIGHTS["base_score_weight"] + WEIGHTS["max_bonus"] * bonus_weight
    
    final_score = base_score * WEIGHTS["base_score_weight"] + bonus_score * bonus_weight
    normalized_score = final_score / max_possible if max_possible > 0 else 0.0
    
    return min(normalized_score, 1.0)  # 1.0ì„ ë„˜ì§€ ì•Šë„ë¡


def calculate_score_with_debug(item, user_inputs, detected_texts=None, debug_ocr=False):
    base_score = item['similarity_score']
    bonus_score = 0.0
    reasons = []
    breakdown = {
        "brand": 0.0,
        "name": 0.0,
        "price": 0.0,
        "ocr": 0.0,
        "ocr_ratio": 0.0
    }

    brand_matched = False
    user_brand = user_inputs.get('brand')
    if user_brand:
        target_maker_keyword = get_official_maker_name(user_brand)
        if target_maker_keyword in item.get('maker', ''):
            bonus_score += WEIGHTS["brand_bonus"]
            breakdown["brand"] = WEIGHTS["brand_bonus"]
            reasons.append(f"brand:+{WEIGHTS['brand_bonus']:.2f}({target_maker_keyword})")
            brand_matched = True
    
    # OCRì—ì„œ ë¸Œëœë“œëª… ë°œê²¬
    if not brand_matched and detected_texts:
        detected_full = ' '.join(detected_texts)
        item_maker = item.get('maker', '')
        matched_word = None
        match_type = None
        # ì™„ì „ ì¼ì¹˜
        if item_maker and item_maker in detected_full:
            bonus_score += WEIGHTS["brand_bonus"]
            breakdown["brand"] = WEIGHTS["brand_bonus"]
            matched_word = item_maker
            match_type = "exact"
            brand_matched = True
        # ìœ ì‚¬ë„ ì²´í¬
        elif item_maker:
            for word in detected_texts:
                if len(word) >= 3:
                    sim = similarity(word, item_maker)
                    if sim >= SIMILARITY_THRESHOLD:
                        bonus_score += WEIGHTS["brand_bonus"]
                        breakdown["brand"] = WEIGHTS["brand_bonus"]
                        matched_word = f"{word}â‰ˆ{item_maker}"
                        match_type = f"{sim:.0%}"
                        brand_matched = True
                        break
        if matched_word:
            reasons.append(f"brand:+{WEIGHTS['brand_bonus']:.2f}(OCR:{matched_word})")

    # ì œí’ˆëª… (name íŒŒë¼ë¯¸í„° ë˜ëŠ” OCR ìë™ ê°ì§€)
    name_matched = False
    user_name = user_inputs.get('name')
    if user_name:
        if user_name.lower() in item.get('name', '').lower():
            bonus_score += WEIGHTS["name_bonus"]
            breakdown["name"] = WEIGHTS["name_bonus"]
            reasons.append(f"name:+{WEIGHTS['name_bonus']:.2f}(API:{user_name})")
            name_matched = True
    
    # OCRì—ì„œ ì œí’ˆëª… ìë™ ê°ì§€
    if not name_matched and detected_texts:
        detected_full = ' '.join(detected_texts)
        item_name = item.get('name', '')
        matched_word = None
        # ì™„ì „ ì¼ì¹˜
        if item_name and (item_name in detected_full or 
                          any(word in item_name for word in detected_texts if len(word) >= 2)):
            bonus_score += WEIGHTS["name_bonus"]
            breakdown["name"] = WEIGHTS["name_bonus"]
            matched_word = next((w for w in detected_texts if len(w) >= 2 and w in item_name), item_name[:10])
            name_matched = True
        # ìœ ì‚¬ë„ ì²´í¬
        elif item_name:
            for word in detected_texts:
                if len(word) >= 3:
                    sim = similarity(word, item_name)
                    if sim >= SIMILARITY_THRESHOLD:
                        bonus_score += WEIGHTS["name_bonus"]
                        breakdown["name"] = WEIGHTS["name_bonus"]
                        matched_word = f"{word}â‰ˆ{item_name[:10]}"
                        name_matched = True
                        break
        if matched_word:
            reasons.append(f"name:+{WEIGHTS['name_bonus']:.2f}(OCR:{matched_word})")

    # ê°€ê²©
    user_price = user_inputs.get('price')
    if user_price:
        try:
            target_price = int(user_price)
            item_price = int(item.get('price', 0))
            diff = abs(target_price - item_price)
            price_ratio = (diff / target_price * 100) if target_price > 0 else 100
            if price_ratio <= WEIGHTS["price_threshold_10pct"]:
                bonus_score += WEIGHTS["price_bonus_10pct"]
                breakdown["price"] = WEIGHTS["price_bonus_10pct"]
                reasons.append(f"price:+{WEIGHTS['price_bonus_10pct']:.2f}(<={WEIGHTS['price_threshold_10pct']:.0f}%)")
            elif price_ratio <= WEIGHTS["price_threshold_20pct"]:
                bonus_score += WEIGHTS["price_bonus_20pct"]
                breakdown["price"] = WEIGHTS["price_bonus_20pct"]
                reasons.append(f"price:+{WEIGHTS['price_bonus_20pct']:.2f}(<={WEIGHTS['price_threshold_20pct']:.0f}%)")
        except Exception:
            pass

    # OCR ì¼ì¹˜ìœ¨ (ì—…ë¡œë“œ ì´ë¯¸ì§€ vs DB ë©”íƒ€ë°ì´í„°)
    if detected_texts:
        match_ratio, ocr_bonus = _calculate_ocr_match_score(
            detected_texts,
            item,
            debug_ocr=debug_ocr,
        )
        breakdown["ocr_ratio"] = match_ratio
        if ocr_bonus > 0:  # ìµœì†Œì¹˜ ì´ìƒì¼ ë•Œë§Œ
            bonus_score += ocr_bonus
            breakdown["ocr"] = ocr_bonus
            if match_ratio >= WEIGHTS["ocr_threshold_good"]:
                level = "ìš°ìˆ˜"
            elif match_ratio >= WEIGHTS["ocr_threshold_fair"]:
                level = "ë³´í†µ"
            else:
                level = "ë¯¸í¡"
            reasons.append(f"ocr:+{ocr_bonus:.2f}({match_ratio:.0f}%-{level})")
        elif match_ratio > 0:
            # ìµœì†Œì¹˜ ë¯¸ë§Œ: ë³´ë„ˆìŠ¤ ì—†ì§€ë§Œ ì¼ì¹˜ìœ¨ ê¸°ë¡
            reasons.append(f"ocr:+0.00({match_ratio:.0f}%-ë¯¸í¡,ìµœì†Œì¹˜ë¯¸ë§Œ)")

    # ì •ê·œí™”: 0~1 ë²”ìœ„ë¡œ ë³€í™˜
    bonus_weight = 1.0 - WEIGHTS["base_score_weight"]
    max_possible = 1.0 * WEIGHTS["base_score_weight"] + WEIGHTS["max_bonus"] * bonus_weight
    
    final_score = base_score * WEIGHTS["base_score_weight"] + bonus_score * bonus_weight
    normalized_score = final_score / max_possible if max_possible > 0 else 0.0
    normalized_score = min(normalized_score, 1.0)  # 1.0ì„ ë„˜ì§€ ì•Šë„ë¡
    
    return normalized_score, reasons, breakdown


def _extract_texts(res):
    if isinstance(res, dict):
        texts = res.get("rec_texts") or res.get("texts")
        if texts:
            return list(texts)
        if "text" in res:
            return [res.get("text")]
        return []

    for attr in ("to_json", "json"):
        if hasattr(res, attr):
            try:
                data = getattr(res, attr)()
                return _extract_texts(data)
            except Exception:
                pass

    if isinstance(res, list):
        texts = []
        for line in res:
            if isinstance(line, (list, tuple)) and len(line) >= 2:
                payload = line[1]
                if isinstance(payload, (list, tuple)) and len(payload) >= 1:
                    text = str(payload[0]).strip()
                    if text:
                        texts.append(text)
        return texts

    return []


def _calculate_ocr_match_score(detected_texts, item, debug_ocr=False):
    """
    ì—…ë¡œë“œ ì´ë¯¸ì§€ì˜ OCR í…ìŠ¤íŠ¸ì™€ DB ìƒí’ˆ ì •ë³´(name, maker, ocr_lines)ì˜ ì¼ì¹˜ìœ¨ ê³„ì‚°
    ë°˜í™˜: (ì¼ì¹˜ìœ¨%, ë³´ë„ˆìŠ¤ ì ìˆ˜)
    """
    # DBì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    db_texts = []
    
    # nameê³¼ maker ì¶”ê°€
    if item.get('name'):
        db_texts.extend(item['name'].split())
    if item.get('maker'):
        db_texts.extend(item['maker'].split())
    
    # ocr_lines íŒŒì‹± (JSON ë¬¸ìì—´)
    ocr_lines_str = item.get('ocr_lines', '[]')
    try:
        ocr_lines = json.loads(ocr_lines_str)
        for line in ocr_lines:
            if isinstance(line, dict) and 'text' in line:
                db_texts.extend(line['text'].split())
    except:
        pass
    
    # ê²¹ì¹˜ëŠ” ë‹¨ì–´ ê³„ì‚° (ì™„ì „ ì¼ì¹˜ + ìœ ì‚¬ë„)
    detected_set = set(w.lower() for w in detected_texts if w)
    db_set = set(w.lower() for w in db_texts if w)
    
    # ì™„ì „ ì¼ì¹˜
    exact_overlap = detected_set & db_set
    overlap_count = len(exact_overlap)
    
    # ìœ ì‚¬ë„ ë§¤ì¹­ (ì™„ì „ ì¼ì¹˜ ëª»í•œ ê²ƒë“¤ë¼ë¦¬)
    remaining_detected = detected_set - exact_overlap
    remaining_db = db_set - exact_overlap
    fuzzy_matches = []
    
    for det_word in remaining_detected:
        if len(det_word) < 3:  # ë„ˆë¬´ ì§§ì€ ë‹¨ì–´ëŠ” skip
            continue
        for db_word in remaining_db:
            if len(db_word) < 3:
                continue
            sim = similarity(det_word, db_word)
            if sim >= SIMILARITY_THRESHOLD:
                fuzzy_matches.append((det_word, db_word, sim))
                overlap_count += 1
                remaining_db.discard(db_word)  # ì¤‘ë³µ ë§¤ì¹­ ë°©ì§€
                break
    
    # ğŸ” DEBUG: OCR ë§¤ì¹­ ê³¼ì • ì¶œë ¥
    if debug_ocr:
        print(f"    ğŸ” OCR DEBUG for {item.get('name', 'Unknown')[:30]}")
        print(f"       Detected: {list(detected_set)[:5]}... (total: {len(detected_set)})")
        print(f"       DB: {list(db_set)[:5]}... (total: {len(db_set)})")
        print(f"       Exact match: {exact_overlap}")
        if fuzzy_matches:
            print(f"       Fuzzy match: {[(d, b, f'{s:.0%}') for d, b, s in fuzzy_matches[:3]]}")
    
    if not detected_set or not db_set:
        return 0.0, 0.0
    
    overlap = overlap_count
    total = max(len(detected_set), len(db_set))
    
    match_ratio = (overlap / total * 100) if total > 0 else 0.0
    
    # 3ë‹¨ê³„ êµ¬ê°„ (ì„ê³„ê°’ì€ .envì—ì„œ ë¡œë“œ)
    # ìµœì†Œì¹˜ ë¯¸ë§Œì´ë©´ ë³´ë„ˆìŠ¤ ë¯¸ì§€ê¸‰
    if match_ratio >= WEIGHTS["ocr_threshold_good"]:
        return match_ratio, WEIGHTS["ocr_bonus_good"]  # ìš°ìˆ˜
    elif match_ratio >= WEIGHTS["ocr_threshold_fair"]:
        return match_ratio, WEIGHTS["ocr_bonus_fair"]  # ë³´í†µ
    elif match_ratio >= WEIGHTS["ocr_threshold_minimum"]:
        return match_ratio, WEIGHTS["ocr_bonus_poor"]  # ë¯¸í¡
    else:
        return match_ratio, 0.0  # ìµœì†Œì¹˜ ë¯¸ë§Œ: ë³´ë„ˆìŠ¤ ì—†ìŒ

# ==========================================
# 3. ë©”ì¸ API: ì´ë¯¸ì§€ ê²€ìƒ‰ (+ í•„í„°)
# ==========================================
@app.post("/search/image")
async def search_by_image(
    file: UploadFile = File(...),      # í•„ìˆ˜: ì´ë¯¸ì§€ íŒŒì¼
    name: Optional[str] = Form(None),  # ì„ íƒ: ì œí’ˆëª…
    price: Optional[str] = Form(None), # ì„ íƒ: ê°€ê²©
    brand: Optional[str] = Form(None)  # ì„ íƒ: ë¸Œëœë“œ
):
    # ëª¨ë¸ ì´ˆê¸°í™” (ì²« ìš”ì²­ ì‹œ í•œ ë²ˆë§Œ)
    initialize_models()
    
    try:
        # 1. ì´ë¯¸ì§€ ì½ê¸°
        image_data = await file.read()
        image = Image.open(io.BytesIO(image_data)).convert("RGB")

        # 2. [ì •ë°€ ëª¨ë“œ] ì—…ë¡œë“œëœ ì´ë¯¸ì§€ OCR ìˆ˜í–‰ (ì†ë„ í¬ìƒ, ì •í™•ë„ UP)
        ocr_result = ocr.predict(input=np.array(image))
        detected_texts = []
        for res in ocr_result:
            detected_texts.extend(_extract_texts(res))
        full_ocr_text = " ".join(detected_texts)
        
        print(f"ğŸ“¸ OCR ê°ì§€ëœ í…ìŠ¤íŠ¸: {full_ocr_text}")

        # 3. ì´ë¯¸ì§€ ë²¡í„° ë³€í™˜
        query_vector = model.encode(image).tolist()

        # 4. 1ì°¨ í›„ë³´êµ° ê²€ìƒ‰ (ë²¡í„°ë¡œ ìƒìœ„ 50ê°œ ê°€ì ¸ì˜´ - ë„‰ë„‰í•˜ê²Œ)
        results = collection.query(
            query_embeddings=[query_vector],
            n_results=50,
            include=["metadatas", "distances", "embeddings"]
        )

        if DEBUG_SCORING:
            print(
                "DEBUG query results:",
                {
                    "ids": len(results.get("ids", [])),
                    "metadatas": len(results.get("metadatas", [])),
                    "distances": len(results.get("distances", [])),
                },
            )

        # 5. 2ì°¨ ì¬ìˆœìœ„í™” (Re-ranking)
        candidates = []
        user_inputs = {"name": name, "price": price, "brand": brand}
        
        ids_list = results.get('ids', [])
        metas_list = results.get('metadatas', [])
        dists_list = results.get('distances', [])
        embeddings_list = results.get('embeddings', [])

        if not ids_list or not ids_list[0]:
            return {
                "status": "success",
                "detected_text": full_ocr_text,
                "results": []
            }

        ids = ids_list[0]
        metadatas = metas_list[0] if metas_list else []
        distances = dists_list[0] if dists_list else []
        embeddings = embeddings_list[0] if embeddings_list else []

        if DEBUG_SCORING:
            print(
                "DEBUG first batch sizes:",
                {
                    "ids": len(ids),
                    "metadatas": len(metadatas),
                    "distances": len(distances),
                    "embeddings": len(embeddings),
                },
            )

        debug_scored = 0
        for item_id, meta, dist, embedding in zip(ids, metadatas, distances, embeddings):
            item = meta
            item['id'] = item_id
            # Cosine similarity (0~1 ë²”ìœ„)
            cosine_dist = cosine(query_vector, embedding)
            item['similarity_score'] = 1 - cosine_dist
            
            # ì—¬ê¸°ì„œ ê°€ì¤‘ì¹˜ ê³„ì‚°! (detected_texts í¬í•¨)
            if DEBUG_SCORING:
                final_score, reasons, breakdown = calculate_score_with_debug(
                    item,
                    user_inputs,
                    detected_texts,
                    debug_ocr=debug_scored < DEBUG_SCORING_LIMIT,
                )
            else:
                final_score = calculate_final_score(item, user_inputs, detected_texts)
                reasons = []
                breakdown = {}

            if DEBUG_SCORING and debug_scored < DEBUG_SCORING_LIMIT:
                print("=" * 80)
                print(f"ğŸ” DEBUG [{debug_scored + 1}/{DEBUG_SCORING_LIMIT}] - {item.get('name', 'Unknown')}")
                print(f"ğŸ“¦ ID: {item_id}")
                print(f"ğŸ¢ Maker: {item.get('maker', 'N/A')}")
                print(f"ğŸ’° Price: {item.get('price', 'N/A')}")
                print("-" * 80)
                print(f"ğŸ“Š Base Score (ë²¡í„° ìœ ì‚¬ë„): {item['similarity_score']:.4f}")
                print(f"ğŸ Bonus Breakdown:")
                print(f"   â€¢ Brand:   +{breakdown.get('brand', 0.0):.3f}")
                print(f"   â€¢ Name:    +{breakdown.get('name', 0.0):.3f}")
                print(f"   â€¢ Price:   +{breakdown.get('price', 0.0):.3f}")
                print(f"   â€¢ OCR:     +{breakdown.get('ocr', 0.0):.3f} (ì¼ì¹˜ìœ¨: {breakdown.get('ocr_ratio', 0.0):.1f}%)")
                print(f"   ğŸ’¡ Total Bonus: {sum([breakdown.get('brand', 0), breakdown.get('name', 0), breakdown.get('price', 0), breakdown.get('ocr', 0)]):.3f}")
                print("-" * 80)
                print(f"â­ Final Score (ì •ê·œí™”): {final_score:.4f}")
                if reasons:
                    print(f"ğŸ“ Reasons: {' | '.join(reasons)}")
                print("=" * 80)
                print()
                debug_scored += 1

            item['final_score'] = final_score
            candidates.append(item)

        # 6. ì ìˆ˜ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬ í›„ ìƒìœ„ 20ê°œ ìë¥´ê¸°
        candidates.sort(key=lambda x: x['final_score'], reverse=True)
        top_20 = candidates[:20]

        response_payload = {
            "status": "success",
            "detected_text": full_ocr_text, # ë””ë²„ê¹…ìš©: OCRì´ ë­˜ ì½ì—ˆëŠ”ì§€ ì•Œë ¤ì¤Œ
            "results": top_20
        }

        with open(RESPONSE_JSON_PATH, "w", encoding="utf-8") as f:
            json.dump(response_payload, f, ensure_ascii=False, indent=2)

        return response_payload

    except Exception as e:
        print("âŒ ì—ëŸ¬ ë°œìƒ:")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import sys
    # reload ëª¨ë“œì—ì„œëŠ” ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë¶ˆí•„ìš”í•œ ëª¨ë¸ ë¡œë“œ ë°©ì§€
    uvicorn.run(
        "server:app",
        host="0.0.0.0",
        port=8000,
        reload=False,
        reload_excludes=["*.pyc", "__pycache__"]
    )